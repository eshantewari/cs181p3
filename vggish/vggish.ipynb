{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "r\"\"\"\n",
    "\n",
    "Pipeline that reads in all wav files in a folder and then outputs them as json objects \n",
    "\n",
    "Usage:\n",
    "  $ python vggish_inference_demo.py \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import json\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this simple example, we run the examples from a single audio file through\n",
    "# the model. If none is provided, we generate a synthetic input.\n",
    "\n",
    "#Read in .wav files from input directory, create array of wav_file names\n",
    "\n",
    "wav_file_direc = \"../WaveFiles/\"\n",
    "embedding_direc = \"../Data/\"\n",
    "\n",
    "checkpoint = \"vggish_model.ckpt\"\n",
    "pca_params = \"vggish_pca_params.npz\"\n",
    "\n",
    "\n",
    "wav_files = listdir(wav_file_direc)\n",
    "\n",
    "#Initialize array of batches and read each wav_file in wav_files array\n",
    "batches = []\n",
    "\n",
    "for wav_file in wav_files:\n",
    "    if \"wav\" in wav_file:\n",
    "        examples_batch = vggish_input.wavfile_to_examples(join(wav_file_direc,wav_file))\n",
    "        batches.append(examples_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Prepare a postprocessor to munge the model embeddings.\n",
    "pproc = vggish_postprocess.Postprocessor(pca_params)\n",
    "\n",
    "output_sequences = []\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    for batch in batches:\n",
    "        # Run inference and postprocessing.\n",
    "        [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                   feed_dict={features_tensor: batch})\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        output_sequences.append(postprocessed_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the Output Sequences, Not Relevant in Full Pipeline\n",
    "order = []\n",
    "for wavfile in wav_files:\n",
    "    order.append(int(wavfile[7:-4]))\n",
    "\n",
    "output_sequences_sorted = []\n",
    "for i in range(0, len(wav_files)):\n",
    "    arg = order.index(i)\n",
    "    output_sequences_sorted.append(output_sequences[arg])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_sequences_sorted = np.array(output_sequences_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the tensor, so that we can load it into other models\n",
    "np.save('../Data/xtrain_vggish', output_sequences_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4, 128)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
